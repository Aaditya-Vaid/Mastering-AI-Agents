{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e2a61f5",
   "metadata": {},
   "source": [
    "# Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bccd72",
   "metadata": {},
   "source": [
    "Memory: Stores and retrieves relevant information across interactions.\n",
    "This component maintains conversation history and context to enable coherent multi-turn interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddda6223",
   "metadata": {},
   "source": [
    "## Groq with Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec97c4ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did Mumbai and Delhi go to therapy together? Because Mumbai was struggling to 'sea' things from a different perspective and Delhi was having trouble 'delivering' on its promises!\n",
      "==================================================\n",
      "This conversation has just started, so you haven't asked me any questions yet. What would you like to talk about?\n",
      "==================================================\n",
      "Your last question was to tell a joke about Indian cities.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from groq import Groq\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = Groq(\n",
    "    api_key= os.getenv(\"GROQ_API_KEY\"),\n",
    ")\n",
    "\n",
    "def ask_joke_without_memory():\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"Answer the questions asked and be creative in you responses. Try to think new jokes everytime. Don't say anything apart from the joke.\"   \n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Tell me a joke about indian cities.\"\n",
    "            }\n",
    "        ],\n",
    "        temperature=0.0\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def ask_follow_up_without_memory():\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"what was my last question?\"\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "    \n",
    "def ask_follow_up_with_memory(joke_response: str):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Tell me a joke about indian cities.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": joke_response\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"what was my last question?\"\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "print(ask_joke_without_memory())\n",
    "print(\"=\"*50)\n",
    "print(ask_follow_up_without_memory())\n",
    "print(\"=\"*50)\n",
    "print(ask_follow_up_with_memory(ask_joke_without_memory()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7af7365",
   "metadata": {},
   "source": [
    "### Groq chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2afac2ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================AI Message========================================\n",
      "Hi! How are you today?\n",
      "========================================Human Message========================================\n",
      "tell me a short joke\n",
      "========================================AI Message========================================\n",
      "Here's one:\n",
      "\n",
      "What do you call a fake noodle?\n",
      "\n",
      "An impasta!\n",
      "\n",
      "Hope that made you smile!\n",
      "========================================Human Message========================================\n",
      "what was my last question\n",
      "========================================AI Message========================================\n",
      "Your last question was to tell you a short joke.\n",
      "========================================Human Message========================================\n",
      "bye\n"
     ]
    }
   ],
   "source": [
    "def groq_chatting(First_input: str):\n",
    "    input_text = First_input\n",
    "    client = Groq(\n",
    "        api_key= os.getenv(\"GROQ_API_KEY\"),\n",
    "    )\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": input_text\n",
    "        },\n",
    "    ]\n",
    "    while(input_text!=\"bye\"):\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "            messages=messages,\n",
    "            # stream=True\n",
    "        )\n",
    "        messages.append({\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": response.choices[0].message.content\n",
    "        })\n",
    "        print(\"=\"*40+\"AI Message\"+\"=\"*40)\n",
    "        print(response.choices[0].message.content)\n",
    "        input_text = input()\n",
    "        messages.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": input_text\n",
    "        })\n",
    "        print(\"=\"*40+\"Human Message\"+\"=\"*40)\n",
    "        print(input_text)\n",
    "        \n",
    "groq_chatting(\"hi\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa321274",
   "metadata": {},
   "source": [
    "## Gemini with memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7a868b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both GOOGLE_API_KEY and GEMINI_API_KEY are set. Using GOOGLE_API_KEY.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did Salman Khan get a speeding ticket?\n",
      "\n",
      "Because he was \"Wanted\" by the police!\n",
      "\n",
      "==================================================\n",
      "I am a language model, I do not have memory of past conversations. I cannot recall your previous question.\n",
      "\n",
      "==================================================\n",
      "Your last question was: **\"tell me a joke about salman khan\"**\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "\n",
    "def ask_joke_without_memory():\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.0-flash-lite\",\n",
    "        contents=[\n",
    "            (\"user\", \"tell me a joke about salman khan\"),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return response.text\n",
    "\n",
    "def ask_follow_up_without_memory():\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.0-flash-lite\",\n",
    "        contents=[\n",
    "            (\"user\", \"what was my last question.\"),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return response.text\n",
    "    \n",
    "def ask_follow_up_with_memory(joke_response: str):\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.0-flash-lite\",\n",
    "        contents=[\n",
    "            (\"user\", \"tell me a joke about salman khan\"),\n",
    "            (\"assistant\", joke_response),\n",
    "            (\"user\", \"what was my last question?\"),\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "    return response.text\n",
    "\n",
    "\n",
    "print(ask_joke_without_memory())\n",
    "print(\"=\"*50)\n",
    "print(ask_follow_up_without_memory())\n",
    "print(\"=\"*50)\n",
    "print(ask_follow_up_with_memory(ask_joke_without_memory()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a174d6",
   "metadata": {},
   "source": [
    "### Gemini chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "801abecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both GOOGLE_API_KEY and GEMINI_API_KEY are set. Using GOOGLE_API_KEY.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================AI Message========================================\n",
      "Hello there! How can I help you today?\n",
      "\n",
      "========================================Human Message========================================\n",
      "what is the integration of exp(x)?\n",
      "========================================AI Message========================================\n",
      "The integral of e<sup>x</sup> (or exp(x)) is simply **e<sup>x</sup> + C**.\n",
      "\n",
      "Where:\n",
      "\n",
      "*   **e<sup>x</sup>** is the exponential function.\n",
      "*   **C** is the constant of integration.  Remember that the derivative of a constant is always zero, so we need to include this to represent all possible antiderivatives.\n",
      "\n",
      "========================================Human Message========================================\n",
      "make me a bowl of ramen noodles\n",
      "========================================AI Message========================================\n",
      "Okay, I can guide you through making a basic bowl of ramen!  Here's a simple recipe and instructions. Since I can't physically cook, I'll have to be your virtual sous chef!\n",
      "\n",
      "**Ingredients You'll Need:**\n",
      "\n",
      "*   **Ramen Noodles:**  The instant kind is easiest, but you can use fresh or dried ramen noodles for a better taste.\n",
      "*   **Water:**  For boiling the noodles.\n",
      "*   **Ramen Seasoning Packet:**  This comes with the instant noodles. (Or, if you're making a more advanced ramen, you'll have ingredients for your own broth.)\n",
      "*   **Optional Toppings (Customize to your liking!):**\n",
      "    *   **Protein:**  Cooked chicken, sliced pork, tofu, or eggs (boiled, poached, or fried).\n",
      "    *   **Vegetables:**  Scallions (green onions), seaweed (nori), mushrooms (shiitake, enoki), bean sprouts, spinach, corn, bamboo shoots.\n",
      "    *   **Other:**  Soy sauce, sesame oil, chili oil, sriracha, a squeeze of lime.\n",
      "\n",
      "**Instructions:**\n",
      "\n",
      "1.  **Boil the Water:** Pour the amount of water specified on the instant noodle package into a pot or saucepan and bring it to a rolling boil. (Usually around 2 cups/500 ml per package). If you are using a different type of noodles, use the appropriate amount of water based on the cooking directions.\n",
      "\n",
      "2.  **Cook the Noodles:** Add the ramen noodles to the boiling water.  Cook them for the time recommended on the package. This is typically 2-3 minutes for instant noodles, but can vary. If using a different type of noodles, use the appropriate cooking time from the directions. Stir occasionally to prevent the noodles from sticking together.\n",
      "\n",
      "3.  **Add the Seasoning (and/or broth components):** While the noodles are cooking, add the seasoning packet (or any broth base ingredients) to the water. Stir well to dissolve the powder. If you want to use homemade broth, add it here instead.\n",
      "\n",
      "4.  **Prepare Your Toppings (If applicable):** While the noodles are cooking, prepare your toppings. If you are using cooked meat, warm it up. If you are using raw vegetables, wash and cut them. Cook the eggs.\n",
      "\n",
      "5.  **Assemble the Ramen:**\n",
      "    *   Carefully pour the noodles and broth into a bowl.\n",
      "    *   Arrange your desired toppings on top.\n",
      "\n",
      "6.  **Season to Taste:** Add any extra soy sauce, chili oil, or other condiments to your liking.\n",
      "\n",
      "7.  **Enjoy!** Slurp up your delicious ramen!\n",
      "\n",
      "**Tips and Variations:**\n",
      "\n",
      "*   **Upgrade your Broth:**  Instead of just the seasoning packet, consider adding a spoonful of miso paste, a splash of soy sauce, or a drizzle of sesame oil to the broth.\n",
      "*   **Add an Egg:** A soft-boiled egg is a classic ramen topping.\n",
      "*   **Get Creative with Vegetables:**  Don't be afraid to experiment with different vegetables and toppings.\n",
      "*   **Fresh Noodles:** If you can get them, fresh ramen noodles have a superior texture.\n",
      "*   **Homemade Broth:** For a truly amazing ramen experience, try making your own broth! This takes more time but is well worth it. Chicken, pork, or vegetable broth are all good starting points.\n",
      "\n",
      "Let me know if you'd like more detailed instructions, variations, or help with specific toppings. Happy cooking!\n",
      "\n",
      "========================================Human Message========================================\n",
      "bye\n"
     ]
    }
   ],
   "source": [
    "def gemini_chatting(First_input: str):\n",
    "    input_text = First_input\n",
    "    client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "    messages = [\n",
    "        (\"user\", input_text),\n",
    "    ]\n",
    "    while(input_text!=\"bye\"):\n",
    "        response = client.models.generate_content(\n",
    "            model=\"gemini-2.0-flash-lite\",\n",
    "            contents=messages\n",
    "        )\n",
    "        messages.append(\n",
    "            (\"assistant\", response.text)\n",
    "        )\n",
    "        print(\"=\"*40+\"AI Message\"+\"=\"*40)\n",
    "        print(response.text)\n",
    "        input_text = input()\n",
    "        messages.append(\n",
    "            (\"user\", input_text)\n",
    "        )\n",
    "        print(\"=\"*40+\"Human Message\"+\"=\"*40)\n",
    "        print(input_text)\n",
    "        \n",
    "gemini_chatting(\"hi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a655db1",
   "metadata": {},
   "source": [
    "# End of the notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
