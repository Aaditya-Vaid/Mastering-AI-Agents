{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe586862",
   "metadata": {},
   "source": [
    "# Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80798020",
   "metadata": {},
   "source": [
    "Tools: Enables agents to execute specific actions in external systems.\n",
    "This component provides the capability to make API calls, database updates, file operations, and other practical actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c3ea6b",
   "metadata": {},
   "source": [
    "## Tool use in Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2a0799d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61c887ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "import json\n",
    "import requests\n",
    "\n",
    "client = Groq(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "MODEL = \"meta-llama/llama-4-scout-17b-16e-instruct\"\n",
    "\n",
    "def calculate(expression: str):\n",
    "    \"\"\"Evaluate a mathematical expression\"\"\"\n",
    "    try:\n",
    "        # Attempt to evaluate the expression\n",
    "        result =  eval(expression)\n",
    "        return json.dumps({\"result\": result})\n",
    "    except Exception as e:\n",
    "        # Return an error message if the expression is invalid\n",
    "        return json.dumps({\"error\": \"Invalid Expression\"})\n",
    "\n",
    "def get_weather(latitude: str, longitude: str):\n",
    "    \"\"\"Get current temperature for provided coordinates in celsius.\"\"\"\n",
    "    response = requests.get(\"https://api.open-meteo.com/v1/forecast?latitude=\"+latitude+\"&longitude=\"+longitude+\"&current=temperature_2m,wind_speed_10m\")\n",
    "    \n",
    "    data = response.json()\n",
    "    temperature = data[\"current\"][\"temperature_2m\"]\n",
    "    print(f\"Temperature: {temperature}°C\")\n",
    "    # Return as JSON string to match the calculate function format\n",
    "    return json.dumps({\"temperature\": temperature, \"unit\": \"celsius\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5fd8ecef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature: 26.1°C\n",
      "[{'role': 'system', 'content': 'You are a helpful agent. You are equiped with some tools use them according to the need.'}, {'role': 'user', 'content': 'what is the temparature of kanpur and after that do this calculation 30*4+100-20'}, ChatCompletionMessage(content=\"To proceed, I need to get the temperature of Kanpur. Kanpur's coordinates are approximately 26.4499° N, 80.3319° E.\\n\\nFor the temperature: \", role='assistant', executed_tools=None, function_call=None, reasoning=None, tool_calls=[ChatCompletionMessageToolCall(id='nc3qh00qw', function=Function(arguments='{\"latitude\":\"26.4499\",\"longitude\":\"80.3319\"}', name='get_weather'), type='function')]), {'tool_call_id': 'nc3qh00qw', 'role': 'tool', 'name': 'get_weather', 'content': '{\"temperature\": 26.1, \"unit\": \"celsius\"}'}]\n",
      "============================================================\n",
      "The current temperature in Kanpur is 26.1°C.\n",
      "\n",
      "Now, let's perform the calculation:\n",
      "\n",
      "30 * 4 = 120\n",
      "120 + 100 = 220\n",
      "220 - 20 = 200\n",
      "\n",
      "The result of the calculation is 200.\n"
     ]
    }
   ],
   "source": [
    "def run_conversation(user_prompt: str):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful agent. You are equiped with some tools use them according to the need.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_prompt\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"calculate\",\n",
    "                \"description\": \"Evaluate a mathematical expression\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"expression\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The mathematical expression to evaluate.\" \n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"expression\"],\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"get_weather\",\n",
    "                \"description\": \"Get current temperature for provided coordinates in celsius.\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"latitude\": {\n",
    "                            \"type\": \"string\"\n",
    "                        },\n",
    "                        \"longitude\": {\n",
    "                            \"type\": \"string\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"latitude\", \"longitude\"]\n",
    "                },\n",
    "                \"strict\": True,\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=messages,\n",
    "        tools = tools,\n",
    "        tool_choice=\"auto\",\n",
    "        max_completion_tokens=4096,\n",
    "    )\n",
    "    \n",
    "    response_message = response.choices[0].message\n",
    "    tool_calls = response_message.tool_calls\n",
    "    if tool_calls:\n",
    "        available_functions = {\n",
    "            \"calculate\": calculate,\n",
    "            \"get_weather\": get_weather,\n",
    "        }\n",
    "        messages.append(response_message)\n",
    "        \n",
    "        for tool_call in tool_calls:\n",
    "            function_name = tool_call.function.name\n",
    "            function_to_call = available_functions[function_name]\n",
    "            function_args = json.loads(tool_call.function.arguments)\n",
    "            \n",
    "            function_response = function_to_call(**function_args)\n",
    "            # function_response = function_to_call(\n",
    "            #     expression=function_args.get(\"expression\")\n",
    "            # )\n",
    "            messages.append(\n",
    "                {\n",
    "                    \"tool_call_id\": tool_call.id,\n",
    "                    \"role\": \"tool\",\n",
    "                    \"name\": function_name,\n",
    "                    \"content\": function_response\n",
    "                }\n",
    "            )\n",
    "        second_response = client.chat.completions.create(\n",
    "            model = MODEL,\n",
    "            messages=messages\n",
    "        )\n",
    "        print(messages)\n",
    "        print(\"=\"*60)  \n",
    "        return second_response.choices[0].message.content\n",
    "    \n",
    "    \n",
    "user_prompt = \"what is the temparature of kanpur and after that do this calculation 30*4+100-20\"\n",
    "print(run_conversation(user_prompt=user_prompt)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e65b60",
   "metadata": {},
   "source": [
    "### Parallel tool use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f7c5042d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current temperature in New York is 24.1°C and the relative humidity is 77%. \n",
      "\n",
      "The current temperature in London is 20.1°C and the relative humidity is 83%.\n"
     ]
    }
   ],
   "source": [
    "def get_temperature(latitude: str, longitude: str):\n",
    "    response = requests.get(\"https://api.open-meteo.com/v1/forecast?latitude=\"+latitude+\"&longitude=\"+longitude+\"&current=temperature_2m\")\n",
    "    data = response.json()\n",
    "    temperature = data['current']['temperature_2m']\n",
    "    return json.dumps({\"temperature\": temperature, \"unit\": \"celsius\"})\n",
    "\n",
    "def get_relative_humidity(latitude: str, longitude: str):\n",
    "    response = requests.get(\"https://api.open-meteo.com/v1/forecast?latitude=\"+latitude+\"&longitude=\"+longitude+\"&current=relative_humidity_2m\")\n",
    "    data = response.json()\n",
    "    humidity = data['current']['relative_humidity_2m']\n",
    "    return json.dumps({\"relative_humidity\": humidity, \"unit\": \"%\"})\n",
    "\n",
    "\n",
    "\n",
    "def run_convo(user_prompt):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful agent. You are equiped with some tools, use them as needed.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_prompt\n",
    "        }\n",
    "    ]\n",
    "    tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"get_temperature\",\n",
    "                \"description\": \"Get current temperature for provided coordinates in celsius.\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"latitude\": {\"type\": \"string\"},\n",
    "                        \"longitude\": {\"type\": \"string\"},\n",
    "                    },\n",
    "                    \"required\": [\"latitude\", \"longitude\"],\n",
    "                },\n",
    "                \"strict\": True,\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"get_relative_humidity\",\n",
    "                \"description\": \"Get current relative humidity for provided coordinates in percentage.\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"latitude\": {\"type\": \"string\"},\n",
    "                        \"longitude\": {\"type\": \"string\"},\n",
    "                    },\n",
    "                    \"required\": [\"latitude\", \"longitude\"],\n",
    "                },\n",
    "                \"strict\": True,\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=messages,\n",
    "        tools=tools,\n",
    "        tool_choice=\"auto\",\n",
    "        max_completion_tokens=4096,\n",
    "    )\n",
    "    \n",
    "    response_message = response.choices[0].message\n",
    "    \n",
    "    tool_calls = response_message.tool_calls\n",
    "    messages.append(response_message)\n",
    "    available_functions = {\n",
    "        \"get_temperature\": get_temperature,\n",
    "        \"get_relative_humidity\": get_relative_humidity,\n",
    "    }\n",
    "    # print(tool_calls)\n",
    "    \n",
    "    for tool_call in tool_calls:\n",
    "        function_name = tool_call.function.name\n",
    "        function_to_call = available_functions[function_name]\n",
    "        function_args = json.loads(tool_call.function.arguments)\n",
    "        function_response = function_to_call(**function_args)\n",
    "        \n",
    "        messages.append(\n",
    "            {\n",
    "                \"role\": \"tool\",\n",
    "                \"content\": str(function_response),\n",
    "                \"tool_call_id\": tool_call.id\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # print(messages)\n",
    "        # print(\"=\"*100)\n",
    "\n",
    "    final_response = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=messages,\n",
    "        tools=tools, \n",
    "        tool_choice=\"auto\", \n",
    "        max_completion_tokens=4096\n",
    "    )\n",
    "    \n",
    "    print(final_response.choices[0].message.content)\n",
    "    \n",
    "user_prompt = \"What's the temperature and relative humidity like in New York and London?\"\n",
    "run_convo(user_prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3b8d8d",
   "metadata": {},
   "source": [
    "## Tool use groq and gemini langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c788cdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def add(a: str, b: str) -> int:\n",
    "    \"\"\"Add two integers a and b.\"\"\"\n",
    "    return int(a) + int(b)\n",
    "\n",
    "@tool\n",
    "def multiply(a: str, b: str) -> int:\n",
    "    \"\"\"Multiply two integers a and b.\"\"\"\n",
    "    return int(a) * int(b)\n",
    "\n",
    "tools = [add, multiply]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9c8677df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "# Note that the docstrings here are crucial, as they will be passed along\n",
    "# to the model along with the class name.\n",
    "class Add(BaseModel):\n",
    "    \"\"\"Add two integers together.\"\"\"\n",
    "\n",
    "    a: str = Field(..., description=\"First integer\")\n",
    "    b: str = Field(..., description=\"Second integer\")\n",
    "\n",
    "\n",
    "class Multiply(BaseModel):\n",
    "    \"\"\"Multiply two integers together.\"\"\"\n",
    "\n",
    "    a: str = Field(..., description=\"First integer\")\n",
    "    b: str = Field(..., description=\"Second integer\")\n",
    "\n",
    "\n",
    "tools = [Add, Multiply]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc36c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm = init_chat_model(model=MODEL, model_provider=\"groq\")\n",
    "# llm = init_chat_model(model='gemini-2.0-flash', model_provider=\"google_genai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "fd6df273",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "48e4d413",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'Add',\n",
       "  'args': {'a': '10', 'b': '2'},\n",
       "  'id': '7ehj757ft',\n",
       "  'type': 'tool_call'},\n",
       " {'name': 'Multiply',\n",
       "  'args': {'a': '11', 'b': '9'},\n",
       "  'id': 'sd74xgqb9',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"what is 10+2? Also, what is 11*9?\"\n",
    "llm_with_tools.invoke(query).tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c17b5ebb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Add(a='10', b='2'), Multiply(a='11', b='9')]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers.openai_tools import PydanticToolsParser\n",
    "\n",
    "chain = llm_with_tools | PydanticToolsParser(tools=[Add, Multiply])\n",
    "chain.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "64350962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='You are bad at math but are an expert at using a calculator. \\n\\nUse past tool usage as an example of how to correctly use the tools.', additional_kwargs={}, response_metadata={}), HumanMessage(content='what is 10+2? Also, what is 11*9?', additional_kwargs={}, response_metadata={}), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'nmee04g9e', 'function': {'arguments': '{\"a\":\"10\",\"b\":\"2\"}', 'name': 'Add'}, 'type': 'function'}, {'id': 'jck6haffb', 'function': {'arguments': '{\"a\":\"11\",\"b\":\"9\"}', 'name': 'Multiply'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 53, 'prompt_tokens': 776, 'total_tokens': 829, 'completion_time': 0.123664831, 'prompt_time': 0.022950529, 'queue_time': 0.046204550999999996, 'total_time': 0.14661536}, 'model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'system_fingerprint': 'fp_79da0e0073', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--b3f116a3-9b9b-4264-ac1f-98c923b0646c-0', tool_calls=[{'name': 'Add', 'args': {'a': '10', 'b': '2'}, 'id': 'nmee04g9e', 'type': 'tool_call'}, {'name': 'Multiply', 'args': {'a': '11', 'b': '9'}, 'id': 'jck6haffb', 'type': 'tool_call'}], usage_metadata={'input_tokens': 776, 'output_tokens': 53, 'total_tokens': 829}), ToolMessage(content='12', tool_call_id='nmee04g9e'), ToolMessage(content='99', tool_call_id='jck6haffb')]\n",
      "====================================================================================================\n",
      "The results are 12 and 99.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, ToolMessage, SystemMessage\n",
    "\n",
    "def run_conversation(human_query):\n",
    "    system_prompt = \"\"\"You are bad at math but are an expert at using a calculator. \n",
    "\n",
    "Use past tool usage as an example of how to correctly use the tools.\"\"\"\n",
    "    messages = [SystemMessage(system_prompt), HumanMessage(human_query)]\n",
    "    ai_messages = llm_with_tools.invoke(messages)\n",
    "    messages.append(ai_messages)\n",
    "    available_tools = {\n",
    "        \"add\": add,\n",
    "        \"multiply\": multiply,\n",
    "    }\n",
    "    for tool_call in ai_messages.tool_calls:\n",
    "        tool_name = tool_call[\"name\"]\n",
    "        selected_tool = available_tools[tool_name.lower()]\n",
    "        tool_output = selected_tool(tool_call[\"args\"])\n",
    "        messages.append(ToolMessage(tool_output, tool_call_id=tool_call[\"id\"]))\n",
    "    \n",
    "    print(messages)\n",
    "    final_messages = llm_with_tools.invoke(messages)\n",
    "    print(\"=\"*100)\n",
    "    print(final_messages.content)\n",
    "\n",
    "run_conversation(query)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e7a633bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='You are bad at math but are an expert at using a calculator. \\n\\nUse past tool usage as an example of how to correctly use the tools.', additional_kwargs={}, response_metadata={}), HumanMessage(content='what is 119 time 8 plus 20?', additional_kwargs={}, response_metadata={}), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': '8fpp2445z', 'function': {'arguments': '{\"a\":\"119\",\"b\":\"8\"}', 'name': 'Multiply'}, 'type': 'function'}, {'id': 'wbjnpsfbg', 'function': {'arguments': '{\"a\":\"952\",\"b\":\"20\"}', 'name': 'Add'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 92, 'prompt_tokens': 770, 'total_tokens': 862, 'completion_time': 0.209401243, 'prompt_time': 0.022741704, 'queue_time': 0.04428341599999999, 'total_time': 0.232142947}, 'model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'system_fingerprint': 'fp_37da608fc1', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--ff2b96d9-fefa-40f1-abdd-6d1880208806-0', tool_calls=[{'name': 'Multiply', 'args': {'a': '119', 'b': '8'}, 'id': '8fpp2445z', 'type': 'tool_call'}, {'name': 'Add', 'args': {'a': '952', 'b': '20'}, 'id': 'wbjnpsfbg', 'type': 'tool_call'}], usage_metadata={'input_tokens': 770, 'output_tokens': 92, 'total_tokens': 862}), ToolMessage(content='952', tool_call_id='8fpp2445z'), ToolMessage(content='972', tool_call_id='wbjnpsfbg')]\n",
      "====================================================================================================\n",
      "The result is 972.\n"
     ]
    }
   ],
   "source": [
    "run_conversation(\"what is 119 time 8 plus 20?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8800c87",
   "metadata": {},
   "source": [
    "## Gemini Function Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "668229b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_light_values(brightness: int, color_temp: int) -> dict[str, int | str]:\n",
    "    \"\"\"Set the brightness and color temperature of a room light. (mock API)\n",
    "    \n",
    "    Args: \n",
    "        brightness: Light level from 0 to 100. Zero is off and 100 is full brightness.\n",
    "        color_temp: Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.\n",
    "        \n",
    "    Returns:\n",
    "        A dictionary containing the set brightness and color temperature.\n",
    "    \"\"\"\n",
    "    return {\"brightness\": brightness, \"colorTemperature\": color_temp}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "9e5be8e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function execution result: {'brightness': 20, 'colorTemperature': 'warm'}\n",
      "[Content(\n",
      "  parts=[\n",
      "    Part(\n",
      "      text='Turn the lights down to a romantic level.'\n",
      "    ),\n",
      "  ],\n",
      "  role='user'\n",
      "), Content(\n",
      "  parts=[\n",
      "    Part(\n",
      "      function_call=FunctionCall(\n",
      "        args={\n",
      "          'brightness': 20,\n",
      "          'color_temp': 'warm'\n",
      "        },\n",
      "        name='set_light_values'\n",
      "      )\n",
      "    ),\n",
      "  ],\n",
      "  role='model'\n",
      "), Content(\n",
      "  parts=[\n",
      "    Part(\n",
      "      function_response=FunctionResponse(\n",
      "        name='set_light_values',\n",
      "        response={\n",
      "          'result': {\n",
      "            'brightness': 20,\n",
      "            'colorTemperature': 'warm'\n",
      "          }\n",
      "        }\n",
      "      )\n",
      "    ),\n",
      "  ],\n",
      "  role='tool'\n",
      ")]\n",
      "================================================================================\n",
      "OK. I've set the lights to a brightness of 20 and a warm color temperature.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "import base64\n",
    "\n",
    "client = genai.Client(\n",
    "    api_key=os.getenv(\"GEMINI_API_KEY\"),\n",
    ")\n",
    "\n",
    "function_declarations = [\n",
    "    {\n",
    "        \"name\": \"set_light_values\",\n",
    "        \"description\": \"Set the brightness and color temperature of a room light\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"brightness\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"description\": \"Light level from 0 to 100. Zero is off and 100 is full brightness\"\n",
    "                },\n",
    "                \"color_temp\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\": [\"daylight\", \"cool\", \"warm\"],\n",
    "                    \"description\": \"Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"brightness\", \"color_temp\"],\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "tools = types.Tool(function_declarations=function_declarations)\n",
    "config = types.GenerateContentConfig(\n",
    "    tools=[tools],\n",
    "    thinking_config= types.ThinkingConfig(\n",
    "        include_thoughts=True,\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "def run_conversation(human_prompt):\n",
    "    content = [\n",
    "        types.Content(\n",
    "            role=\"user\", parts=[types.Part(text=human_prompt)]\n",
    "        )\n",
    "    ]\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.0-flash\",\n",
    "        contents=content,\n",
    "        config=config\n",
    "    )\n",
    "    \n",
    "    tool_call = response.candidates[0].content.parts[0].function_call\n",
    "    \n",
    "    if tool_call.name == \"set_light_values\":\n",
    "        result = set_light_values(**tool_call.args)\n",
    "        print(f\"Function execution result: {result}\")\n",
    "    \n",
    "    function_response_part = types.Part.from_function_response(\n",
    "        name = tool_call.name,\n",
    "        response = {\"result\": result}\n",
    "    )\n",
    "    \n",
    "    content.append(response.candidates[0].content)\n",
    "    thought_signature = response.candidates[0].content.parts[0].thought_signature\n",
    "    if thought_signature:\n",
    "        print(f\"Thought Signature: {base64.b64encode(thought_signature).decode(\"utf-8\")}\")\n",
    "    content.append(types.Content(role=\"tool\", parts=[function_response_part]))\n",
    "    \n",
    "    final_response = client.models.generate_content(\n",
    "        model=\"gemini-2.0-flash\",\n",
    "        contents= content,\n",
    "        config=config\n",
    "    )\n",
    "    print(content)\n",
    "    print(\"=\"*80)\n",
    "    print(final_response.text)\n",
    "\n",
    "run_conversation(\"Turn the lights down to a romantic level.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf6dce7",
   "metadata": {},
   "source": [
    "### Parallel function calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "935babc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_disco_ball_impl(power: bool) -> dict:\n",
    "    \"\"\"Powers the spinning disco ball.\n",
    "\n",
    "    Args:\n",
    "        power: Whether to turn the disco ball on or off.\n",
    "\n",
    "    Returns:\n",
    "        A status dictionary indicating the current state.\n",
    "    \"\"\"\n",
    "    return {\"status\": f\"Disco ball powered {'on' if power else 'off'}\"}\n",
    "\n",
    "def start_music_impl(energetic: bool, loud: bool) -> dict:\n",
    "    \"\"\"Play some music matching the specified parameters.\n",
    "\n",
    "    Args:\n",
    "        energetic: Whether the music is energetic or not.\n",
    "        loud: Whether the music is loud or not.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the music settings.\n",
    "    \"\"\"\n",
    "    music_type = \"energetic\" if energetic else \"chill\"\n",
    "    volume = \"loud\" if loud else \"quiet\"\n",
    "    return {\"music_type\": music_type, \"volume\": volume}\n",
    "\n",
    "def dim_lights_impl(brightness: float) -> dict:\n",
    "    \"\"\"Dim the lights.\n",
    "\n",
    "    Args:\n",
    "        brightness: The brightness of the lights, 0.0 is off, 1.0 is full.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the new brightness setting.\n",
    "    \"\"\"\n",
    "    return {\"brightness\": brightness}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "2a4751dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1: Forced function calling\n",
      "power_disco_ball(power=True)\n",
      "start_music(energetic=True, loud=True)\n",
      "dim_lights(brightness=0.5)\n"
     ]
    }
   ],
   "source": [
    "client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "function_declarations = [\n",
    "    {\n",
    "        \"name\": \"power_disco_ball\",\n",
    "        \"description\": \"Powers the spinning disco ball.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"power\": {\n",
    "                    \"type\": \"boolean\",\n",
    "                    \"description\": \"Whether to turn the disco ball on or off.\",\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"power\"],\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"start_music\",\n",
    "        \"description\": \"Play some music matching the specified parameters.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"energetic\": {\n",
    "                    \"type\": \"boolean\",\n",
    "                    \"description\": \"Whether the music is energetic or not.\",\n",
    "                },\n",
    "                \"loud\": {\n",
    "                    \"type\": \"boolean\",\n",
    "                    \"description\": \"Whether the music is loud or not.\",\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"energetic\", \"loud\"],\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"dim_lights\",\n",
    "        \"description\": \"Dim the lights.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"brightness\": {\n",
    "                    \"type\": \"number\",\n",
    "                    \"description\": \"The brightness of the lights, 0.0 is off, 1.0 is full.\",\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"brightness\"],\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "house_tools = types.Tool(function_declarations=function_declarations)\n",
    "config = types.GenerateContentConfig(\n",
    "    tools = [house_tools],\n",
    "    automatic_function_calling=types.AutomaticFunctionCallingConfig(\n",
    "        disable=True\n",
    "    ),\n",
    "    tool_config=types.ToolConfig(\n",
    "        function_calling_config=types.FunctionCallingConfig(\n",
    "            mode=\"ANY\",\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "chat = client.chats.create(\n",
    "    model='gemini-2.0-flash',\n",
    "    config=config,\n",
    ")\n",
    "response = chat.send_message(\"Turn this place into a party.\")\n",
    "# Print out each of the function calls requested from this single call\n",
    "print(\"Example 1: Forced function calling\")\n",
    "for fn in response.function_calls:\n",
    "    args = \", \".join(f\"{key}={val}\" for key, val in fn.args.items())\n",
    "    print(f\"{fn.name}({args})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "ee475114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 2: Automatic function calling\n",
      "Alright, I've got the disco ball spinning, the music is loud and energetic, and the lights are dimmed. The party is officially started!\n"
     ]
    }
   ],
   "source": [
    "config = types.GenerateContentConfig(\n",
    "    tools=[power_disco_ball_impl, start_music_impl, dim_lights_impl]\n",
    ")\n",
    "\n",
    "# Make the request\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=\"Do everything you need to this place into party!\",\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "print(\"\\nExample 2: Automatic function calling\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5695187b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
