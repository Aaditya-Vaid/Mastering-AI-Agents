{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82da79bb",
   "metadata": {},
   "source": [
    "# Recovery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5504c3c",
   "metadata": {},
   "source": [
    "Recovery: Manages failures and exceptions gracefully in agent workflows.\n",
    "This component implements retry logic, fallback processes, and error handling to ensure system resilience.\n",
    "\n",
    "We will also work with python libraries like:\n",
    "* Tenacity: a library for retrying functions, and\n",
    "* PyBreaker: a library for Circuit Breaker pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd746032",
   "metadata": {},
   "source": [
    "## Recovery with Groq and Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9cbfe61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional\n",
    "import json\n",
    "from groq import RateLimitError, APIError, APIConnectionError\n",
    "import time\n",
    "import random\n",
    "from httpx import Response, Request\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a9cd3d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt:1/4 to process request...\n",
      "SIMULATING: RateLimitError for attempt 1...\n",
      "RateLimitError detected: Simulated Rate Limit Exceeded. Retrying in 1.00 seconds...\n",
      "Attempt:2/4 to process request...\n",
      "SIMULATING: RateLimitError for attempt 2...\n",
      "RateLimitError detected: Simulated Rate Limit Exceeded. Retrying in 2.48 seconds...\n",
      "Attempt:3/4 to process request...\n",
      "SIMULATING: RateLimitError for attempt 3...\n",
      "RateLimitError detected: Simulated Rate Limit Exceeded. Retrying in 5.15 seconds...\n",
      "Attempt:4/4 to process request...\n",
      "--- Successful Response ---\n",
      "\n",
      "Final Answer for Prompt 1:\n",
      "{\n",
      "  \"name\": \"Jane Smith\",\n",
      "  \"email\": \"jane.s@web.org\",\n",
      "  \"age\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "client = Groq(\n",
    "    api_key=os.getenv(\"GROQ_API_KEY\"),\n",
    ")\n",
    "\n",
    "MAX_RETRIES = 4\n",
    "INITIAL_BACKOFF_SECONDS = 1\n",
    "MAX_BACKOFF_SECONDS = 60\n",
    "\n",
    "class UserInfo(BaseModel):\n",
    "    name: str = Field(description=\"Name of the user\")\n",
    "    age: Optional[int] = Field(default=None, description= \"Age of the user\")\n",
    "    email: str = Field(description=\"Email of the user\")\n",
    "\n",
    "def process_client_request(prompt:str, model: str = \"meta-llama/llama-4-scout-17b-16e-instruct\", max_tokens = 50):\n",
    "    retries = 0\n",
    "    current_backoff_seconds = INITIAL_BACKOFF_SECONDS\n",
    "    \n",
    "    while retries<MAX_RETRIES:\n",
    "        try:\n",
    "            print(f\"Attempt:{retries+1}/{MAX_RETRIES} to process request...\")\n",
    "            # --- START: ARTIFICIAL RATE LIMIT SIMULATION ---\n",
    "            if retries < 3: # Simulate RateLimitError for the first 3 attempts\n",
    "                print(f\"SIMULATING: RateLimitError for attempt {retries+1}...\")\n",
    "\n",
    "                # Create a dummy httpx.Request object\n",
    "                dummy_request = Request(method=\"POST\", url=\"http://api.groq.com/dummy-endpoint\")\n",
    "\n",
    "                # Create a dummy httpx.Response object with the dummy request\n",
    "                dummy_response = Response(\n",
    "                    status_code=429,\n",
    "                    request=dummy_request, # <-- Now passing the dummy request\n",
    "                    headers={\"Retry-After\": \"60\"} # Often present for 429 errors\n",
    "                )\n",
    "                # Create a dummy body string (often JSON)\n",
    "                dummy_body = json.dumps({\"error\": {\"message\": \"You are being rate limited.\", \"type\": \"rate_limit_error\"}})\n",
    "\n",
    "                # Raise the RateLimitError correctly\n",
    "                raise RateLimitError(\"Simulated Rate Limit Exceeded\", response=dummy_response, body=dummy_body)\n",
    "            # --- END: ARTIFICIAL RATE LIMIT SIMULATION ---\n",
    "            messages = [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"Extract information from the text.\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }\n",
    "            ]\n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages = messages,\n",
    "                response_format={\n",
    "                    \"type\": \"json_schema\",\n",
    "                    \"json_schema\": {\n",
    "                        \"name\": \"user_info\",\n",
    "                        \"schema\": UserInfo.model_json_schema()\n",
    "                    }\n",
    "                },\n",
    "                temperature=0.7,\n",
    "                max_tokens=max_tokens\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except RateLimitError as e:\n",
    "            # \"Error detected\" -> Retry possible? -> (Yes, for RateLimitError)\n",
    "            print(f\"RateLimitError detected: {e}. Retrying in {current_backoff_seconds:.2f} seconds...\")\n",
    "            time.sleep(current_backoff_seconds)\n",
    "            retries+=1\n",
    "            current_backoff_seconds = min(2*current_backoff_seconds+ random.uniform(0,1), MAX_BACKOFF_SECONDS)\n",
    "            \n",
    "        except (APIConnectionError, APIError) as e:\n",
    "            # \"Error Detected\" -> \"Retry Possible?\" (Yes, for transient API errors/connection issues)\n",
    "            print(f\"API Error or Connection Error detected: {e}. Retrying in {current_backoff_seconds:.2f} seconds...\")\n",
    "            time.sleep(current_backoff_seconds)\n",
    "            retries += 1\n",
    "            current_backoff_seconds = min(current_backoff_seconds * 2 + random.uniform(0, 1), MAX_BACKOFF_SECONDS)\n",
    "        \n",
    "        except Exception as e:\n",
    "            # Catch any other unexpected errors. \"Error Detected\" -> \"Retry Possible?\" (No, for unknown/non-retryable errors)\n",
    "            print(f\"An unexpected error occurred: {e}. This might be a non-retryable error.\")\n",
    "            break\n",
    "        \n",
    "    print(f\"Failed to get a successful response after {MAX_RETRIES} attempts.\")\n",
    "    return None \n",
    "\n",
    "def get_answer_with_fallback(prompt: str)-> str:\n",
    "    response = process_client_request(prompt)\n",
    "    if response is not None:\n",
    "        print(\"--- Successful Response ---\")\n",
    "        return response\n",
    "    else :\n",
    "        # this is the \"fallback\" part\n",
    "        print(\"--- Executing Fallback ---\")\n",
    "        fallback_message = \"I am sorry, I couldn't process your request at this time. Please try again later or contact support.\"\n",
    "        print(f\"Fallback Message: {fallback_message}\")\n",
    "        # return fallback_message\n",
    "    \n",
    "    \n",
    "prompt1 = \"Please extract information about the user from this text: Name: Jane Smith, Email: jane.s@web.org\"\n",
    "final_answer1 = get_answer_with_fallback(prompt1)\n",
    "print(f\"\\nFinal Answer for Prompt 1:\\n{final_answer1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5969fbe8",
   "metadata": {},
   "source": [
    "## Tenacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b32c81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    stop_after_delay,\n",
    "    wait_exponential,\n",
    "    retry_if_exception_type\n",
    ")\n",
    "MAX_ATTEMPTS = 5\n",
    "MAX_DELAY_SECONDS = 60\n",
    "INITIAL_BACKOFF_SECONDS=1\n",
    "MAX_BACKOFF_SECONDS=60\n",
    "@retry(\n",
    "    stop=stop_after_attempt(MAX_ATTEMPTS)|stop_after_delay(MAX_DELAY_SECONDS),\n",
    "    wait=wait_exponential(multiplier=INITIAL_BACKOFF_SECONDS,max=MAX_BACKOFF_SECONDS, min=INITIAL_BACKOFF_SECONDS),\n",
    "    retry=retry_if_exception_type((RateLimitError, APIConnectionError, APIError)),\n",
    ")\n",
    "def process_client_request_with_tenacity(prompt:str, model: str = \"meta-llama/llama-4-scout-17b-16e-instruct\", max_tokens = 50):\n",
    "    client = Groq(\n",
    "        api_key=os.getenv(\"GROQ_API_KEY\"),\n",
    "    )\n",
    "    print(\"Attempt to process request...\")\n",
    "    print(process_client_request_with_tenacity.statistics)\n",
    "    current_attempt = process_client_request_with_tenacity.statistics.get('attempt_number', 1)\n",
    "    \n",
    "    if current_attempt <= 3: # Accessing tenacity's internal stats\n",
    "        print(f\"SIMULATING: RateLimitError for attempt {current_attempt}...\")\n",
    "        dummy_request = Request(method=\"POST\", url=\"http://api.groq.com/dummy-endpoint\")\n",
    "        dummy_response = Response(\n",
    "            status_code=429,\n",
    "            request=dummy_request,\n",
    "            headers={\"Retry-After\": \"60\"}\n",
    "        )\n",
    "        dummy_body = json.dumps({\"error\": {\"message\": \"You are being rate limited.\", \"type\": \"rate_limit_error\"}})\n",
    "        raise RateLimitError(\"Simulated Rate Limit Exceeded\", response=dummy_response, body=dummy_body)\n",
    "    else:\n",
    "        print(f\"SIMULATION BYPASSED: Attempt {current_attempt}, proceeding with actual Groq call.\")\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Extract information from the text.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt\n",
    "        }\n",
    "    ]\n",
    "    response= client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        response_format={\n",
    "            \"type\": \"json_schema\",\n",
    "            \"json_schema\": {\n",
    "                \"name\": \"user_info\",\n",
    "                \"schema\": UserInfo.model_json_schema()\n",
    "            }\n",
    "        },\n",
    "        temperature=0.7,\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def get_answer_with_fallback_tenacity(prompt: str) -> str:\n",
    "    try:\n",
    "        # Call the tenacity-decorated function\n",
    "        response = process_client_request_with_tenacity(prompt)\n",
    "        print(\"--- Successful Response ---\")\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        # This catch-all exception handles cases where tenacity gives up\n",
    "        print(f\"--- Executing Fallback due to error after retries: {e} ---\")\n",
    "        fallback_message = \"I am sorry, I couldn't process your request at this time. Please try again later or contact support.\"\n",
    "        print(f\"Fallback Message: {fallback_message}\")\n",
    "        return fallback_message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee7561bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt to process request...\n",
      "{'start_time': 89291.25, 'attempt_number': 1, 'idle_for': 0}\n",
      "SIMULATING: RateLimitError for attempt 1...\n",
      "Attempt to process request...\n",
      "{'start_time': 89291.25, 'attempt_number': 2, 'idle_for': 1.0, 'delay_since_first_attempt': 0.6399999999994179}\n",
      "SIMULATING: RateLimitError for attempt 2...\n",
      "Attempt to process request...\n",
      "{'start_time': 89291.25, 'attempt_number': 3, 'idle_for': 3.0, 'delay_since_first_attempt': 2.375}\n",
      "SIMULATING: RateLimitError for attempt 3...\n",
      "Attempt to process request...\n",
      "{'start_time': 89291.25, 'attempt_number': 4, 'idle_for': 7.0, 'delay_since_first_attempt': 5.0460000000020955}\n",
      "SIMULATION BYPASSED: Attempt 4, proceeding with actual Groq call.\n",
      "--- Successful Response ---\n",
      "\n",
      "Final Answer for Prompt 1:\n",
      "{\n",
      "  \"name\": \"Jane Smith\",\n",
      "  \"email\": \"jane.s@web.org\",\n",
      "  \"age\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "prompt2 = \"Please extract information about the user from this text: Name: Jane Smith, Email: jane.s@web.org\"\n",
    "final_answer2 = get_answer_with_fallback_tenacity(prompt2)\n",
    "print(f\"\\nFinal Answer for Prompt 1:\\n{final_answer2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b30bcf",
   "metadata": {},
   "source": [
    "## Recovery with Langchain: RunnableRetry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19095e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.runnables.retry import RunnableRetry\n",
    "\n",
    "\n",
    "\n",
    "llm = init_chat_model(model=\"meta-llama/llama-4-scout-17b-16e-instruct\", model_provider=\"groq\")\n",
    "\n",
    "\n",
    "def process_client_request_with_langchain(prompt: str, model: str = \"meta-llama/llama-4-scout-17b-16e-instruct\", max_tokens=50):\n",
    "    llm = init_chat_model(model=model, model_provider=\"groq\")\n",
    "    \n",
    "    print(\"Attempt to process request...\")\n",
    "    \n",
    "    # Fix: Use a manual counter since this function isn't decorated with @retry\n",
    "    # You can either use a global counter or pass it as a parameter\n",
    "    if not hasattr(process_client_request_with_langchain, 'attempt_counter'):\n",
    "        process_client_request_with_langchain.attempt_counter = 0\n",
    "    \n",
    "    process_client_request_with_langchain.attempt_counter += 1\n",
    "    current_attempt = process_client_request_with_langchain.attempt_counter\n",
    "    \n",
    "    print(f\"Current attempt: {current_attempt}\")\n",
    "    \n",
    "    if current_attempt <= 3:  # Simulate failures for first 3 attempts\n",
    "        print(f\"SIMULATING: RateLimitError for attempt {current_attempt}...\")\n",
    "        dummy_request = Request(method=\"POST\", url=\"http://api.groq.com/dummy-endpoint\")\n",
    "        dummy_response = Response(\n",
    "            status_code=429,\n",
    "            request=dummy_request,\n",
    "            headers={\"Retry-After\": \"60\"}\n",
    "        )\n",
    "        dummy_body = json.dumps({\"error\": {\"message\": \"You are being rate limited.\", \"type\": \"rate_limit_error\"}})\n",
    "        raise RateLimitError(\"Simulated Rate Limit Exceeded\", response=dummy_response, body=dummy_body)\n",
    "    else:\n",
    "        print(f\"SIMULATION BYPASSED: Attempt {current_attempt}, proceeding with actual Groq call.\")\n",
    "    \n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Extract information from the text.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt\n",
    "        }\n",
    "    ]\n",
    "    llm_with_structured_output = llm.with_structured_output(UserInfo)\n",
    "    response = llm_with_structured_output.invoke(messages)\n",
    "    return response\n",
    "\n",
    "def get_answer_with_fallback_langchain(prompt: str) -> str:\n",
    "    runnable = RunnableLambda(process_client_request_with_langchain)\n",
    "\n",
    "\n",
    "    runnable_with_retries = RunnableRetry(\n",
    "        bound=runnable,\n",
    "        retry_exception_types=(RateLimitError, APIConnectionError, APIError),\n",
    "        max_attempt_number=4,\n",
    "        wait_exponential_jitter=True,\n",
    "        exponential_jitter_params={\"initial\": 2},\n",
    "    )\n",
    "    try:\n",
    "        # Call the tenacity-decorated function\n",
    "        response = runnable_with_retries.invoke(prompt)\n",
    "        print(\"--- Successful Response ---\")\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        # This catch-all exception handles cases where tenacity gives up\n",
    "        print(f\"--- Executing Fallback due to error after retries: {e} ---\")\n",
    "        fallback_message = \"I am sorry, I couldn't process your request at this time. Please try again later or contact support.\"\n",
    "        print(f\"Fallback Message: {fallback_message}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "098a4bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt to process request...\n",
      "Current attempt: 1\n",
      "SIMULATING: RateLimitError for attempt 1...\n",
      "Attempt to process request...\n",
      "Current attempt: 2\n",
      "SIMULATING: RateLimitError for attempt 2...\n",
      "Attempt to process request...\n",
      "Current attempt: 3\n",
      "SIMULATING: RateLimitError for attempt 3...\n",
      "Attempt to process request...\n",
      "Current attempt: 4\n",
      "SIMULATION BYPASSED: Attempt 4, proceeding with actual Groq call.\n",
      "--- Successful Response ---\n",
      "\n",
      "Final Answer for Prompt 1:\n",
      "name='Jane Smith' age=None email='jane.s@web.org'\n"
     ]
    }
   ],
   "source": [
    "prompt3 = \"Please extract information about the user from this text: Name: Jane Smith, Email: jane.s@web.org\"\n",
    "final_answer3 = get_answer_with_fallback_langchain(prompt3)\n",
    "print(f\"\\nFinal Answer for Prompt 1:\\n{final_answer3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35006f15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
